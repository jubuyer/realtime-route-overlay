{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JszgNDzPqqWQ"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Environment setup\n",
        "!nvidia-smi\n",
        "\n",
        "# Install core dependencies (PyTorch + tools)\n",
        "!pip install --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install --quiet opencv-python tqdm onnx onnxruntime-gpu\n",
        "!pip install addict scikit-learn pathspec imagesize ujson tensorboard\n",
        "!pip install onnxscript"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Clone UFLDv2 repo and move into it\n",
        "%cd /content\n",
        "!git clone https://github.com/cfzd/Ultra-Fast-Lane-Detection-v2.git\n",
        "%cd Ultra-Fast-Lane-Detection-v2\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "JSx632Y7xoc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Download KITTI raw subset for benchmarking\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "DATASETS_ROOT = ROOT / \"datasets\"\n",
        "DOWNLOADS_DIR = DATASETS_ROOT / \"downloads\"\n",
        "KITTI_RAW_DIR = DATASETS_ROOT / \"kitti_raw\"\n",
        "\n",
        "for d in [DATASETS_ROOT, DOWNLOADS_DIR, KITTI_RAW_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Small subset of KITTI raw sequences (you can add more later)\n",
        "kitti_urls = [\n",
        "    \"https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0015/2011_09_26_drive_0015_sync.zip\",\n",
        "    \"https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0027/2011_09_26_drive_0027_sync.zip\",\n",
        "]\n",
        "\n",
        "def download_file(url, out_dir):\n",
        "    local_path = out_dir / url.split(\"/\")[-1]\n",
        "    if local_path.exists():\n",
        "        print(f\"[SKIP] Already exists: {local_path.name}\")\n",
        "        return local_path\n",
        "\n",
        "    print(f\"\\nDownloading: {url}\")\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        total = int(r.headers.get(\"content-length\", 0))\n",
        "        block_size = 1024\n",
        "        with open(local_path, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=local_path.name) as pbar:\n",
        "            for chunk in r.iter_content(block_size):\n",
        "                f.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "    return local_path\n",
        "\n",
        "def extract_zip(zip_path, extract_to):\n",
        "    print(f\"Extracting {zip_path.name} -> {extract_to}\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(extract_to)\n",
        "\n",
        "for url in kitti_urls:\n",
        "    zip_file = download_file(url, DOWNLOADS_DIR)\n",
        "    extract_zip(zip_file, KITTI_RAW_DIR)\n",
        "\n",
        "print(\"\\nKITTI raw subset downloaded to:\", KITTI_RAW_DIR)\n",
        "!find datasets/kitti_raw -maxdepth 2 -type d | head\n"
      ],
      "metadata": {
        "id": "uyFs4DWqxyvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Create weights directory and upload Tusimple ResNet18\n",
        "from pathlib import Path\n",
        "\n",
        "WEIGHTS_DIR = Path(\"weights\")\n",
        "WEIGHTS_DIR.mkdir(exist_ok=True)\n"
      ],
      "metadata": {
        "id": "VCIqjEE1zlBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Ultra-Fast-Lane-Detection-v2"
      ],
      "metadata": {
        "id": "gtlbGI3m4LT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Preprocess KITTI frames for UFLDv2 benchmark\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import configs.tusimple_res18 as cfg  # UFLDv2 config for Tusimple ResNet18\n",
        "\n",
        "IMG_W, IMG_H = cfg.train_width, cfg.train_height\n",
        "print(\"Using input resolution:\", IMG_W, \"x\", IMG_H)\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "KITTI_RAW_DIR = ROOT / \"datasets\" / \"kitti_raw\" / \"2011_09_26\"\n",
        "BENCH_IMG_DIR = ROOT / \"datasets\" / \"processed\" / \"kitti_ufld_benchmark\" / \"images\"\n",
        "BENCH_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Select drives we downloaded\n",
        "drive_names = [\n",
        "    \"2011_09_26_drive_0015_sync\",\n",
        "    \"2011_09_26_drive_0027_sync\",\n",
        "]\n",
        "\n",
        "all_img_paths = []\n",
        "for drive in drive_names:\n",
        "    drive_dir = KITTI_RAW_DIR / drive / \"image_02\" / \"data\"\n",
        "    if not drive_dir.exists():\n",
        "        print(\"Warning: drive not found:\", drive_dir)\n",
        "        continue\n",
        "    imgs = sorted(drive_dir.glob(\"*.png\"))\n",
        "    all_img_paths.extend(imgs)\n",
        "\n",
        "print(f\"Total raw KITTI frames found: {len(all_img_paths)}\")\n",
        "\n",
        "# Optionally limit number of frames for faster benchmarking\n",
        "MAX_FRAMES = 500\n",
        "all_img_paths = all_img_paths[:MAX_FRAMES]\n",
        "print(f\"Using {len(all_img_paths)} frames for benchmark preprocessing.\")\n",
        "\n",
        "def resize_and_save(src_paths, out_dir, img_w, img_h):\n",
        "    for p in tqdm(src_paths, desc=\"Resizing KITTI frames\"):\n",
        "        img = cv2.imread(str(p))\n",
        "        if img is None:\n",
        "            continue\n",
        "        img_resized = cv2.resize(img, (img_w, img_h))\n",
        "        out_path = out_dir / f\"{p.stem}.png\"\n",
        "        cv2.imwrite(str(out_path), img_resized)\n",
        "\n",
        "resize_and_save(all_img_paths, BENCH_IMG_DIR, IMG_W, IMG_H)\n",
        "\n",
        "print(\"Preprocessed KITTI benchmark images saved to:\", BENCH_IMG_DIR)\n",
        "!ls datasets/processed/kitti_ufld_benchmark/images | head\n"
      ],
      "metadata": {
        "id": "V4yQjaOt1wwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark cell with DALI stub + get_model(cfg)\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Make sure we're in repo root\n",
        "%cd /content/Ultra-Fast-Lane-Detection-v2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1) Build a proper fake nvidia.dali package hierarchy (so imports don't fail)\n",
        "# -------------------------------------------------------------------\n",
        "import sys, types\n",
        "\n",
        "# Base \"nvidia\" package\n",
        "nvidia_mod = sys.modules.get(\"nvidia\")\n",
        "if nvidia_mod is None:\n",
        "    nvidia_mod = types.ModuleType(\"nvidia\")\n",
        "    nvidia_mod.__path__ = []\n",
        "    sys.modules[\"nvidia\"] = nvidia_mod\n",
        "\n",
        "# \"nvidia.dali\" package\n",
        "dali_mod = sys.modules.get(\"nvidia.dali\")\n",
        "if dali_mod is None:\n",
        "    dali_mod = types.ModuleType(\"nvidia.dali\")\n",
        "    dali_mod.__path__ = []\n",
        "    sys.modules[\"nvidia.dali\"] = dali_mod\n",
        "\n",
        "# \"nvidia.dali.plugin\" package\n",
        "plugin_mod = sys.modules.get(\"nvidia.dali.plugin\")\n",
        "if plugin_mod is None:\n",
        "    plugin_mod = types.ModuleType(\"nvidia.dali.plugin\")\n",
        "    plugin_mod.__path__ = []\n",
        "    sys.modules[\"nvidia.dali.plugin\"] = plugin_mod\n",
        "\n",
        "# \"nvidia.dali.plugin.pytorch\" module\n",
        "pytorch_mod = sys.modules.get(\"nvidia.dali.plugin.pytorch\")\n",
        "if pytorch_mod is None:\n",
        "    pytorch_mod = types.ModuleType(\"nvidia.dali.plugin.pytorch\")\n",
        "\n",
        "    class DummyDALIGenericIterator:\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            pass\n",
        "\n",
        "    class DummyLastBatchPolicy:\n",
        "        DROP = 0\n",
        "        PARTIAL = 1\n",
        "\n",
        "    pytorch_mod.DALIGenericIterator = DummyDALIGenericIterator\n",
        "    pytorch_mod.LastBatchPolicy = DummyLastBatchPolicy\n",
        "\n",
        "    sys.modules[\"nvidia.dali.plugin.pytorch\"] = pytorch_mod\n",
        "\n",
        "# \"nvidia.dali.types\" and \"nvidia.dali.fn\" stubs\n",
        "types_mod = sys.modules.get(\"nvidia.dali.types\")\n",
        "if types_mod is None:\n",
        "    types_mod = types.ModuleType(\"nvidia.dali.types\")\n",
        "    sys.modules[\"nvidia.dali.types\"] = types_mod\n",
        "\n",
        "fn_mod = sys.modules.get(\"nvidia.dali.fn\")\n",
        "if fn_mod is None:\n",
        "    fn_mod = types.ModuleType(\"nvidia.dali.fn\")\n",
        "    sys.modules[\"nvidia.dali.fn\"] = fn_mod\n",
        "\n",
        "# Link submodules as attributes on dali_mod\n",
        "dali_mod.plugin = plugin_mod\n",
        "dali_mod.types = types_mod\n",
        "dali_mod.fn = fn_mod\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2) Import config and model (use get_model(cfg) instead of parsingNet(...))\n",
        "# -------------------------------------------------------------------\n",
        "import configs.tusimple_res18 as cfg\n",
        "from model import model_tusimple\n",
        "\n",
        "# This is the correct way for this repo:\n",
        "# model_tusimple.get_model(cfg) internally builds parsingNet with all proper args\n",
        "model = model_tusimple.get_model(cfg)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Prepare benchmark images\n",
        "# -------------------------------------------------------------------\n",
        "IMG_W, IMG_H = cfg.train_width, cfg.train_height\n",
        "print(\"Benchmarking with resolution:\", IMG_W, \"x\", IMG_H)\n",
        "\n",
        "BENCH_IMG_DIR = Path(\"datasets/processed/kitti_ufld_benchmark/images\")\n",
        "img_paths = sorted(BENCH_IMG_DIR.glob(\"*.png\"))\n",
        "print(\"Benchmark frames:\", len(img_paths))\n",
        "assert len(img_paths) > 0, \"No preprocessed KITTI frames found!\"\n",
        "\n",
        "def preprocess_tensor(path):\n",
        "    img = cv2.imread(str(path))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    tensor = torch.from_numpy(img).unsqueeze(0)\n",
        "    return tensor.to(device)\n",
        "\n",
        "tensors = [preprocess_tensor(p) for p in img_paths]\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4) Warm-up\n",
        "# -------------------------------------------------------------------\n",
        "with torch.no_grad():\n",
        "    for t in tensors[:10]:\n",
        "        _ = model(t)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5) Timed loop\n",
        "# -------------------------------------------------------------------\n",
        "NUM_PASSES = 3  # repeat passes to smooth noise\n",
        "total_frames = 0\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(NUM_PASSES):\n",
        "        for t in tensors:\n",
        "            _ = model(t)\n",
        "            total_frames += 1\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "end = time.time()\n",
        "\n",
        "total_time = end - start\n",
        "avg_time = total_time / total_frames\n",
        "fps = 1.0 / avg_time\n",
        "\n",
        "print(f\"Total frames processed: {total_frames}\")\n",
        "print(f\"Total time: {total_time:.3f} s\")\n",
        "print(f\"Average latency: {avg_time * 1000:.3f} ms/frame\")\n",
        "print(f\"Throughput: {fps:.2f} FPS on {device}\")\n"
      ],
      "metadata": {
        "id": "vKpxghmw_0hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Export UFLDv2 Tusimple ResNet18 to ONNX for QNN later\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "model.eval()\n",
        "\n",
        "dummy_input = torch.randn(1, 3, IMG_H, IMG_W, device=device)\n",
        "\n",
        "onnx_path = Path(\"weights\") / f\"ufldv2_tusimple_res18_{IMG_W}x{IMG_H}.onnx\"\n",
        "print(\"Exporting ONNX to:\", onnx_path)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path.as_posix(),\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    opset_version=18,\n",
        "    do_constant_folding=True,\n",
        ")\n",
        "\n",
        "print(\"ONNX export complete.\")\n",
        "\n",
        "# Optional: quick sanity check with ONNX Runtime\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "sess = ort.InferenceSession(onnx_path.as_posix(), providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
        "print(\"ONNX Runtime inputs:\", sess.get_inputs()[0].name, sess.get_inputs()[0].shape)\n",
        "\n",
        "# Test one frame\n",
        "import cv2\n",
        "test_img = cv2.imread(str(img_paths[0]))\n",
        "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "test_img = np.transpose(test_img, (2, 0, 1))[None, ...]\n",
        "\n",
        "ort_out = sess.run(None, {\"input\": test_img})[0]\n",
        "print(\"ONNX output shape:\", ort_out.shape)\n"
      ],
      "metadata": {
        "id": "R5H3OoHFCY3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}