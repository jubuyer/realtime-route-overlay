{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZLjfqgWUElo"
      },
      "outputs": [],
      "source": [
        "# Cell 0: Basic setup (run once)\n",
        "!pip install --quiet \"qai-hub\" pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: Configure Qualcomm AI Hub and list available devices\n",
        "import qai_hub as hub\n",
        "\n",
        "# IMPORTANT: put your actual API token here (from AI Hub settings page)\n",
        "hub.set_session_token(\"API token\")  # TODO: replace\n",
        "\n",
        "devices = hub.get_devices()\n",
        "print(\"Available devices:\")\n",
        "for i, d in enumerate(devices):\n",
        "    print(f\"[{i}] {d}\")\n"
      ],
      "metadata": {
        "id": "71vRzvpGV03P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B: Create folder structure for model + calibration data (no copying)\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"/content\")  # Colab root\n",
        "AIHUB_DIR = ROOT / \"aihub\"\n",
        "MODEL_DIR = AIHUB_DIR / \"model\"\n",
        "CALIB_DIR = AIHUB_DIR / \"calib\"\n",
        "\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CALIB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Created (or confirmed) directories:\")\n",
        "print(\"MODEL_DIR:\", MODEL_DIR)\n",
        "print(\"CALIB_DIR:\", CALIB_DIR)\n",
        "\n",
        "# Show empty tree (will be filled after you upload files)\n",
        "!apt-get -qq install tree >/dev/null\n",
        "!tree -L 3 /content/aihub\n"
      ],
      "metadata": {
        "id": "4Sca0KxuV5Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C: Wrap ONNX + .data into ONNX directory container for AI Hub\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "ROOT = Path(\"/content/aihub\")\n",
        "\n",
        "# If your exported model has a different name, change this:\n",
        "MODEL_NAME = \"ufldv2_tusimple_res18_800x320.onnx\"\n",
        "\n",
        "MODEL_UPLOAD_DIR = ROOT / \"model\"\n",
        "onnx_file = MODEL_UPLOAD_DIR / MODEL_NAME\n",
        "data_file = MODEL_UPLOAD_DIR / (MODEL_NAME + \".data\")\n",
        "\n",
        "assert onnx_file.exists(), f\"ONNX file not found: {onnx_file}\"\n",
        "assert data_file.exists(), f\"External weights file not found: {data_file}\"\n",
        "\n",
        "# This is the ONNX \"container\" directory that AI Hub expects:\n",
        "#   /content/aihub/ufldv2_tusimple_res18_800x320.onnx/\n",
        "#     ├── ufldv2_tusimple_res18_800x320.onnx\n",
        "#     └── ufldv2_tusimple_res18_800x320.onnx.data\n",
        "CONTAINER_DIR = ROOT / MODEL_NAME\n",
        "CONTAINER_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "shutil.copy2(onnx_file, CONTAINER_DIR / onnx_file.name)\n",
        "shutil.copy2(data_file, CONTAINER_DIR / data_file.name)\n",
        "\n",
        "print(\"Created ONNX container dir:\", CONTAINER_DIR)\n",
        "!apt-get -qq install tree >/dev/null\n",
        "!tree -L 2 /content/aihub\n"
      ],
      "metadata": {
        "id": "cT1YYojtdnqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C: Select target Snapdragon device\n",
        "\n",
        "import qai_hub as hub\n",
        "\n",
        "devices = hub.get_devices()\n",
        "\n",
        "DEVICE_INDEX = 61\n",
        "\n",
        "device = devices[DEVICE_INDEX]\n",
        "print(\"Selected device:\", device)\n"
      ],
      "metadata": {
        "id": "GN7Q2qoNWpmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell D: Build calibration_data dict from uploaded KITTI frames\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "CALIB_DIR = Path(\"/content/aihub/calib\")\n",
        "IMG_W, IMG_H = 800, 320\n",
        "\n",
        "png_paths = sorted(CALIB_DIR.glob(\"*.png\"))\n",
        "print(\"Found\", len(png_paths), \"calibration images in\", CALIB_DIR)\n",
        "\n",
        "assert len(png_paths) > 0, \"No calibration PNGs found. Upload some images first.\"\n",
        "\n",
        "# Limit number of calibration samples if many\n",
        "MAX_CALIB = 200\n",
        "png_paths = png_paths[:MAX_CALIB]\n",
        "print(\"Using\", len(png_paths), \"images for calibration.\")\n",
        "\n",
        "calib_samples = []\n",
        "for p in png_paths:\n",
        "    img = Image.open(p).convert(\"RGB\")\n",
        "    arr = np.array(img).astype(np.float32) / 255.0\n",
        "    arr = np.transpose(arr, (2, 0, 1))[None, ...]\n",
        "    calib_samples.append(arr)\n",
        "\n",
        "calibration_data = {\"input\": calib_samples}\n",
        "print(\"Built calibration_data with\", len(calibration_data[\"input\"]), \"samples.\")\n"
      ],
      "metadata": {
        "id": "FsphCnXtXjD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell E: Compile and profile FP32 ONNX model on Snapdragon via AI Hub\n",
        "import qai_hub as hub\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"/content/aihub\")\n",
        "MODEL_NAME = \"ufldv2_tusimple_res18_800x320.onnx\"\n",
        "\n",
        "# ONNX *directory container* created in Cell B3\n",
        "CONTAINER_DIR = ROOT / MODEL_NAME\n",
        "assert CONTAINER_DIR.is_dir(), f\"ONNX container dir not found: {CONTAINER_DIR}\"\n",
        "\n",
        "print(\"Submitting FP32 compile job with ONNX container:\", CONTAINER_DIR)\n",
        "\n",
        "compile_fp32_job = hub.submit_compile_job(\n",
        "    model=CONTAINER_DIR,\n",
        "    device=device,\n",
        "    options=\"--target_runtime qnn_context_binary\",\n",
        "    name=\"ufldv2_tusimple_fp32_compile\",\n",
        ")\n",
        "\n",
        "# Wait for completion\n",
        "compile_fp32_job.wait()\n",
        "\n",
        "# Get detailed status\n",
        "status = compile_fp32_job.get_status()\n",
        "print(\"FP32 compile job status code:\", status.code)\n",
        "print(\"FP32 compile job message:\", status.message)\n",
        "print(\"Job URL:\", compile_fp32_job.url)\n",
        "\n",
        "# If status.code is not \"SUCCESS\", bail out early\n",
        "if status.code != \"SUCCESS\":\n",
        "    raise RuntimeError(\n",
        "        f\"Compile job failed with code={status.code}, message={status.message}. \"\n",
        "        f\"Open this URL in your browser for details: {compile_fp32_job.url}\"\n",
        "    )\n",
        "\n",
        "# Otherwise, fetch the compiled target model\n",
        "compiled_fp32_model = compile_fp32_job.get_target_model()\n",
        "if compiled_fp32_model is None:\n",
        "    raise RuntimeError(\n",
        "        \"Compile job reported SUCCESS but returned no target model. \"\n",
        "        f\"Check logs at: {compile_fp32_job.url}\"\n",
        "    )\n",
        "\n",
        "print(\"✅ FP32 compile job done. Target model id:\", compiled_fp32_model.model_id)\n",
        "\n",
        "# Now profile on-device\n",
        "print(\"Submitting FP32 profile job...\")\n",
        "profile_fp32_job = hub.submit_profile_job(\n",
        "    model=compiled_fp32_model,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Block until the profile job finishes\n",
        "profile_status = profile_fp32_job.wait()\n",
        "print(\"FP32 profile job status:\", profile_status)\n",
        "print(\"FP32 profile job URL:\", profile_fp32_job.url)\n",
        "\n",
        "# Download the profile results as a Python dict\n",
        "profile_fp32 = profile_fp32_job.download_profile()\n",
        "print(\"\\n=== FP32 Snapdragon Profile (raw dict) ===\")\n",
        "print(profile_fp32)\n"
      ],
      "metadata": {
        "id": "EEiRRuJbYyCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell F: Quantize (INT8 PTQ), compile, and profile on Snapdragon\n",
        "import qai_hub as hub\n",
        "from pathlib import Path\n",
        "\n",
        "# Assumes:\n",
        "# - calibration_data is already built in Cell E\n",
        "# - device is selected in Cell C\n",
        "# - ONNX container dir created in Cell B3\n",
        "\n",
        "ROOT = Path(\"/content/aihub\")\n",
        "MODEL_NAME = \"ufldv2_tusimple_res18_800x320.onnx\"\n",
        "CONTAINER_DIR = ROOT / MODEL_NAME\n",
        "assert CONTAINER_DIR.is_dir(), f\"ONNX container dir not found: {CONTAINER_DIR}\"\n",
        "\n",
        "# 1) Quantize FP32 ONNX -> INT8 (w8a8) using calibration_data\n",
        "print(\"Submitting INT8 quantize job (w8a8 PTQ)...\")\n",
        "quant_job = hub.submit_quantize_job(\n",
        "    model=CONTAINER_DIR,\n",
        "    calibration_data=calibration_data,\n",
        "    weights_dtype=hub.QuantizeDtype.INT8,\n",
        "    activations_dtype=hub.QuantizeDtype.INT8,\n",
        "    name=\"ufldv2_tusimple_int8_w8a8\",\n",
        ")\n",
        "\n",
        "quant_job.wait()\n",
        "q_status = quant_job.get_status()\n",
        "print(\"INT8 quantize job status code:\", q_status.code)\n",
        "print(\"INT8 quantize job message:\", q_status.message)\n",
        "print(\"INT8 quantize job URL:\", quant_job.url)\n",
        "\n",
        "if q_status.code != \"SUCCESS\":\n",
        "    raise RuntimeError(\n",
        "        f\"Quantize job failed with code={q_status.code}, message={q_status.message}. \"\n",
        "        f\"Open this URL in your browser for details: {quant_job.url}\"\n",
        "    )\n",
        "\n",
        "quantized_onnx_model = quant_job.get_target_model()\n",
        "if quantized_onnx_model is None:\n",
        "    raise RuntimeError(\n",
        "        \"Quantize job reported SUCCESS but returned no target model. \"\n",
        "        f\"Check logs at: {quant_job.url}\"\n",
        "    )\n",
        "\n",
        "print(\"✅ INT8 quantize job done. Quantized model id:\", quantized_onnx_model.model_id)\n",
        "\n",
        "# 2) Compile quantized ONNX to QNN context binary with quantized IO\n",
        "print(\"\\nSubmitting INT8 compile job...\")\n",
        "compile_int8_job = hub.submit_compile_job(\n",
        "    model=quantized_onnx_model,\n",
        "    device=device,\n",
        "    input_specs=dict(input=(1, 3, 320, 800)),\n",
        "    options=\"--target_runtime qnn_context_binary --quantize_io\",\n",
        "    name=\"ufldv2_tusimple_int8_compile\",\n",
        ")\n",
        "\n",
        "compile_int8_job.wait()\n",
        "c_status = compile_int8_job.get_status()\n",
        "print(\"INT8 compile job status code:\", c_status.code)\n",
        "print(\"INT8 compile job message:\", c_status.message)\n",
        "print(\"INT8 compile job URL:\", compile_int8_job.url)\n",
        "\n",
        "if c_status.code != \"SUCCESS\":\n",
        "    raise RuntimeError(\n",
        "        f\"INT8 compile job failed with code={c_status.code}, message={c_status.message}. \"\n",
        "        f\"Open this URL in your browser for details: {compile_int8_job.url}\"\n",
        "    )\n",
        "\n",
        "compiled_int8_model = compile_int8_job.get_target_model()\n",
        "if compiled_int8_model is None:\n",
        "    raise RuntimeError(\n",
        "        \"INT8 compile job reported SUCCESS but returned no target model. \"\n",
        "        f\"Check logs at: {compile_int8_job.url}\"\n",
        "    )\n",
        "\n",
        "print(\"✅ INT8 compile job done. Target model id:\", compiled_int8_model.model_id)\n",
        "\n",
        "# 3) Profile INT8 model on-device\n",
        "print(\"\\nSubmitting INT8 profile job...\")\n",
        "profile_int8_job = hub.submit_profile_job(\n",
        "    model=compiled_int8_model,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "profile_status_int8 = profile_int8_job.wait()\n",
        "print(\"INT8 profile job status:\", profile_status_int8)\n",
        "print(\"INT8 profile job URL:\", profile_int8_job.url)\n",
        "\n",
        "# Download the profile results as a Python dict\n",
        "profile_int8 = profile_int8_job.download_profile()\n",
        "print(\"\\n=== INT8 Snapdragon Profile (raw dict) ===\")\n",
        "print(profile_int8)\n"
      ],
      "metadata": {
        "id": "kEZH0G3DmrHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell G: Compare key performance metrics between FP32 and INT8 profiles\n",
        "\n",
        "def _extract_summary(profile_dict):\n",
        "    \"\"\"\n",
        "    Handle both possible formats:\n",
        "      - profile[\"execution_summary\"][...]\n",
        "      - profile[...] directly\n",
        "    \"\"\"\n",
        "    if isinstance(profile_dict, dict) and \"execution_summary\" in profile_dict:\n",
        "        return profile_dict[\"execution_summary\"]\n",
        "    return profile_dict\n",
        "\n",
        "def _get_latency_us(summary):\n",
        "    \"\"\"Try both estimated_inference_time and execution_time (docs show both variants).\"\"\"\n",
        "    if summary is None:\n",
        "        return None\n",
        "    if \"estimated_inference_time\" in summary:\n",
        "        return summary[\"estimated_inference_time\"]\n",
        "    if \"execution_time\" in summary:\n",
        "        return summary[\"execution_time\"]\n",
        "    return None\n",
        "\n",
        "def _get_peak_mem_bytes(summary):\n",
        "    if summary is None:\n",
        "        return None\n",
        "    # Prefer estimated_inference_peak_memory if present\n",
        "    if \"estimated_inference_peak_memory\" in summary:\n",
        "        return summary[\"estimated_inference_peak_memory\"]\n",
        "    # Fall back to inference_memory_peak_range upper bound if present\n",
        "    if \"inference_memory_peak_range\" in summary and summary[\"inference_memory_peak_range\"]:\n",
        "        return summary[\"inference_memory_peak_range\"][1]\n",
        "    return None\n",
        "\n",
        "def _ms(us):\n",
        "    return us / 1000.0 if us is not None else None\n",
        "\n",
        "def _mb(bytes_):\n",
        "    return bytes_ / (1024.0 * 1024.0) if bytes_ is not None else None\n",
        "\n",
        "# ---- Extract summaries ----\n",
        "summary_fp32 = _extract_summary(profile_fp32)\n",
        "summary_int8 = _extract_summary(profile_int8)\n",
        "\n",
        "lat_us_fp32  = _get_latency_us(summary_fp32)\n",
        "lat_us_int8  = _get_latency_us(summary_int8)\n",
        "mem_b_fp32   = _get_peak_mem_bytes(summary_fp32)\n",
        "mem_b_int8   = _get_peak_mem_bytes(summary_int8)\n",
        "\n",
        "lat_ms_fp32  = _ms(lat_us_fp32)\n",
        "lat_ms_int8  = _ms(lat_us_int8)\n",
        "mem_mb_fp32  = _mb(mem_b_fp32)\n",
        "mem_mb_int8  = _mb(mem_b_int8)\n",
        "\n",
        "# Throughput in FPS from single-batch latency\n",
        "fps_fp32 = 1000.0 / lat_ms_fp32 if lat_ms_fp32 else None\n",
        "fps_int8 = 1000.0 / lat_ms_int8 if lat_ms_int8 else None\n",
        "\n",
        "print(\"=== Snapdragon Performance Comparison (QNN context binary) ===\\n\")\n",
        "\n",
        "def fmt(x, digits=3):\n",
        "    return f\"{x:.{digits}f}\" if isinstance(x, (int, float)) and x is not None else \"N/A\"\n",
        "\n",
        "print(f\"{'Metric':<25} {'FP32':>12} {'INT8 (w8a8)':>15}\")\n",
        "print(\"-\" * 54)\n",
        "print(f\"{'Latency (ms)':<25} {fmt(lat_ms_fp32):>12} {fmt(lat_ms_int8):>15}\")\n",
        "print(f\"{'Throughput (FPS)':<25} {fmt(fps_fp32):>12} {fmt(fps_int8):>15}\")\n",
        "print(f\"{'Peak infer mem (MB)':<25} {fmt(mem_mb_fp32):>12} {fmt(mem_mb_int8):>15}\")\n",
        "\n",
        "# Optional: also show first/warm load times if present\n",
        "fl_us_fp32 = summary_fp32.get(\"first_load_time\") if summary_fp32 else None\n",
        "fl_us_int8 = summary_int8.get(\"first_load_time\") if summary_int8 else None\n",
        "wl_us_fp32 = summary_fp32.get(\"warm_load_time\") if summary_fp32 else None\n",
        "wl_us_int8 = summary_int8.get(\"warm_load_time\") if summary_int8 else None\n",
        "\n",
        "if fl_us_fp32 or fl_us_int8 or wl_us_fp32 or wl_us_int8:\n",
        "    print(\"\\nLoad-time details (ms):\")\n",
        "    print(f\"{'First load':<25} {fmt(_ms(fl_us_fp32)):>12} {fmt(_ms(fl_us_int8)):>15}\")\n",
        "    print(f\"{'Warm load':<25} {fmt(_ms(wl_us_fp32)):>12} {fmt(_ms(wl_us_int8)):>15}\")\n"
      ],
      "metadata": {
        "id": "OItsB-euuBPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}